{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b88583c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Malk\n",
      "[nltk_data]     al\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import all the used libaries\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from random import shuffle\n",
    "from copy import deepcopy\n",
    "\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f5af82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Canon_PowerShot_SD500.txt', 'Canon_S100.txt', 'Diaper_Champ.txt', 'Hitachi_router.txt', 'Linksys_Router.txt', 'MicroMP3.txt', 'Nokia_6600.txt', 'ipod.txt', 'norton.txt']\n"
     ]
    }
   ],
   "source": [
    "#  read data from files \n",
    "corpus_root = 'product_reviews/product_reviews'\n",
    "#  load all txt files except README.txt\n",
    "wordlists = PlaintextCorpusReader(corpus_root, '(?!README.txt$)(.+\\.txt)')\n",
    "allFiles = wordlists.fileids()\n",
    "print(allFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14dc1a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 't', ']', 'software', '[-', '3', ']##', 'I', 'have', 'read', 'the', 'installation', 'instructions', 'for', 'both', 'NIS', '2004', 'and', 'NAV', '2004']\n"
     ]
    }
   ],
   "source": [
    "# display word sample form the documents\n",
    "print(wordlists.words(\"norton.txt\")[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1377648",
   "metadata": {},
   "source": [
    "# Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92ada4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3358\n",
      "['software', 'read', 'installation', 'instruction', 'ni', 'nav', 'prior', 'installation', 'still', 'ended']\n"
     ]
    }
   ],
   "source": [
    "def process_document(document: list) -> list:\n",
    "        \"\"\"\n",
    "        pre-process a document and return a list of tokens\n",
    "        list->list\"\"\"\n",
    "        wordsList=[]\n",
    "#         tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        #keep alphanumeric characters\n",
    "        tokenizer = nltk.RegexpTokenizer(r'[a-zA-Z]{1,}')\n",
    "        all_stopwords = stopwords.words('english')\n",
    "        for w in document:\n",
    "            filtired = tokenizer.tokenize(w.lower())\n",
    "            try:\n",
    "                # checks if a word is a stop words\n",
    "                if not filtired[0] in all_stopwords:\n",
    "                    # added the word to the list after lemmatization \n",
    "                    wordsList.append(lemmatizer.lemmatize(filtired[0]))\n",
    "            except:\n",
    "                ...\n",
    "        return wordsList\n",
    "\n",
    "#prints a samplew of the tokenized words\n",
    "print(len(process_document(wordlists.words(\"norton.txt\"))))\n",
    "print(process_document(wordlists.words(\"norton.txt\"))[:10])\n",
    " \n",
    "# replace most frequent words and return a new list\n",
    "def replace_words(indxes, target_words, tokenized_docs):\n",
    "    new_docs = deepcopy(tokenized_docs)\n",
    "    #loop over all most frequent words indexes\n",
    "    for i in range(len(indxes)):\n",
    "        \n",
    "        # shuffle and split the list by half\n",
    "        middel = len(indxes[i])//2\n",
    "        temp_indxes = indxes[i].copy()\n",
    "        shuffle(temp_indxes)\n",
    "        \n",
    "        #replace half of the word occorances\n",
    "        for index in temp_indxes[:middel]:\n",
    "            x , y = index\n",
    "            #print(i,x, y)\n",
    "            new_docs[x][y] = str(target_words[i+50])\n",
    "    \n",
    "    return new_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9baa4c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38937\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "tokenized_words = [] # this list is only used to callculate most_frequent words\n",
    "tokenized_docs = []\n",
    "\n",
    "for file in wordlists.fileids():\n",
    "    tokenized_words += process_document(wordlists.words(file))\n",
    "#     tokenized_docs.append(process_document(wordlists.words(file)))\n",
    "    tokenized_docs.append( np.array( process_document(wordlists.words(file) )) )\n",
    "\n",
    "print(len(tokenized_words))\n",
    "print(len(tokenized_docs))\n",
    "\n",
    "# print(tokenized_words[:10])\n",
    "# print(tokenized_docs[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "352b0841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_words  100\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "# callculate most_frequent words\n",
    "\n",
    "most_frequent = FreqDist(tokenized_words).most_common(50)\n",
    "# print(most_frequen)\n",
    "\n",
    "# create pseudowords\n",
    "pseudowords = []\n",
    "for word in most_frequent:\n",
    "#     print(word[0],\" -> \" ,word[0][::-1])\n",
    "    pseudowords.append(word[0][::-1])\n",
    "\n",
    "#target_words are the 100 words in the matrix \n",
    "target_words = []\n",
    "for word in most_frequent:\n",
    "    target_words.append(word[0])\n",
    "    \n",
    "target_words = target_words + pseudowords\n",
    "print(\"target_words \",len(target_words))\n",
    "\n",
    "\n",
    "# \n",
    "# gets most frequent words indexes\n",
    "# \n",
    "indxes = []\n",
    "for word in target_words[:50]:\n",
    "    temp = []\n",
    "    for i in range(len(tokenized_docs)):\n",
    "        for j in np.where(tokenized_docs[i] == word)[0]:\n",
    "#             print(i,j)\n",
    "            temp.append([i,j])\n",
    "            \n",
    "    indxes.append(temp)\n",
    "\n",
    "# print(tokenized_sents[1])\n",
    "print(len(indxes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ffe56ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_matrixX(target_words, tokenized_docs):\n",
    "    # Number of documents in the collection that contain the 100 words\n",
    "    small_m = [None for i in range(len(target_words))]\n",
    "\n",
    "    for i in range(len(target_words)):\n",
    "        count = 0\n",
    "        for doc in tokenized_docs:\n",
    "            if target_words[i] in doc:\n",
    "                count+=1\n",
    "        small_m[i] = count\n",
    "\n",
    "    # print(small_m)\n",
    "\n",
    "    matrixX = np.empty([len(most_frequent)*2, len(tokenized_docs)])\n",
    "#     print(\"matrixX shape: \", len(matrixX), len(matrixX[0]))\n",
    "    # print(matrixX)\n",
    "\n",
    "    for i in range(len(target_words)):\n",
    "        for j in range(len(tokenized_docs)): \n",
    "            # calculate tf \n",
    "            tf = np.count_nonzero(tokenized_docs[j] == target_words[i]) / len(tokenized_docs[j])\n",
    "            #tf = tokenized_docs[j].tolist().count(target_words[i]) / len(tokenized_docs[j])\n",
    "\n",
    "            # calculate idf\n",
    "            #Total number of documents / Number of documents that contain the word +1\n",
    "            idf = math.log( (len(tokenized_docs)/(small_m[i]+1) )) +1\n",
    "\n",
    "            matrixX[i][j] = tf*idf\n",
    "\n",
    "    return matrixX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d12778a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "empty clusters:  0\n",
      "correct pairs:  25\n",
      "p:  50.0 %\n",
      "\n",
      "empty clusters:  0\n",
      "correct pairs:  25\n",
      "p:  50.0 %\n",
      "\n",
      "empty clusters:  0\n",
      "correct pairs:  28\n",
      "p:  56.00000000000001 %\n",
      "\n",
      "empty clusters:  0\n",
      "correct pairs:  27\n",
      "p:  54.0 %\n",
      "\n",
      "empty clusters:  0\n",
      "correct pairs:  26\n",
      "p:  52.0 %\n",
      "\n",
      "empty clusters:  0\n",
      "correct pairs:  27\n",
      "p:  54.0 %\n",
      "\n",
      "empty clusters:  0\n",
      "correct pairs:  28\n",
      "p:  56.00000000000001 %\n",
      "\n",
      "empty clusters:  0\n",
      "correct pairs:  22\n",
      "p:  44.0 %\n",
      "\n",
      "empty clusters:  0\n",
      "correct pairs:  32\n",
      "p:  64.0 %\n",
      "\n",
      "empty clusters:  0\n",
      "correct pairs:  28\n",
      "p:  56.00000000000001 %\n",
      "\n",
      "mean:  53.6\n",
      "standard deviation:  4.963869458396343\n"
     ]
    }
   ],
   "source": [
    " # repeat the experiment  10 time\n",
    "N = 10\n",
    "pList = []\n",
    "for i in range(N):\n",
    "    # returns a new list with random replaces words\n",
    "    new_docs = replace_words(indxes, target_words, tokenized_docs)\n",
    "\n",
    "    #create the matrix \n",
    "    matrixX = create_matrixX(target_words, new_docs)\n",
    "    \n",
    "    #normalize the matrix by dividing by the l2 norm\n",
    "    feature_norms = np.linalg.norm(matrixX, axis = 0)\n",
    "    normalized = matrixX/feature_norms\n",
    "\n",
    "    # clester the matrix\n",
    "    kmeans = KMeans(n_clusters=50, random_state=0).fit(normalized)\n",
    "    pred = kmeans.labels_\n",
    "#     print(pred[:50])\n",
    "#     print(pred[50:])\n",
    "\n",
    "    # checks for empty clusters\n",
    "    empty_clusters = []\n",
    "    for i in range(50):\n",
    "        if i not in pred:\n",
    "            empty_clusters.append(i)\n",
    "\n",
    "    print(\"\\nempty clusters: \", len(empty_clusters))\n",
    "    #     print(empty_clusters)\n",
    "\n",
    "    # count correct clustring \n",
    "    count = 0\n",
    "    for i in range(50):\n",
    "        if(pred[i]==pred[i+50]):\n",
    "            count+=1\n",
    "\n",
    "    print(\"correct pairs: \",count)\n",
    "    p = (count/50)*100\n",
    "    print(\"p: \", p, \"%\")\n",
    "    \n",
    "    pList.append(p)\n",
    "\n",
    "print()\n",
    "# calculate the mean and standard deviation for P\n",
    "mean = np.mean(pList)\n",
    "std = np.std(pList)\n",
    "print(\"mean: \", mean)\n",
    "print(\"standard deviation: \", std)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
