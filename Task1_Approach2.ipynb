{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fe4a814",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Malk\n",
      "[nltk_data]     al\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import all the used libaries\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.cluster import KMeans\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from random import shuffle\n",
    "from copy import deepcopy\n",
    "\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e600dc0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Canon_PowerShot_SD500.txt', 'Canon_S100.txt', 'Diaper_Champ.txt', 'Hitachi_router.txt', 'Linksys_Router.txt', 'MicroMP3.txt', 'Nokia_6600.txt', 'ipod.txt', 'norton.txt']\n"
     ]
    }
   ],
   "source": [
    "#  read data from files \n",
    "corpus_root = 'product_reviews/product_reviews'\n",
    "#  load all txt files except README.txt\n",
    "wordlists = PlaintextCorpusReader(corpus_root, '(?!README.txt$)(.+\\.txt)')\n",
    "allFiles = wordlists.fileids()\n",
    "\n",
    "# print files names\n",
    "print(allFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21f743c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 't', ']', 'software', '[-', '3', ']##', 'I', 'have', 'read', 'the', 'installation', 'instructions', 'for', 'both', 'NIS', '2004', 'and', 'NAV', '2004', 'prior', 'to', 'installation', ',', 'but', 'still', 'ended', 'up', 'with', 'the', 'same', 'result', '...', 'junk', 'software', '.'] \n",
      "\n",
      "379\n",
      "[array(['software', 'read', 'installation', 'instruction', 'ni', 'nav',\n",
      "       'prior', 'installation', 'still', 'ended', 'result', 'junk',\n",
      "       'software'], dtype='<U12'), array(['install', 'type', 'software', 'installs', 'work', 'properly'],\n",
      "      dtype='<U8')]\n"
     ]
    }
   ],
   "source": [
    "# returns a list of tokenized words\n",
    "def tokenize_words(document: list) -> list:\n",
    "        \"\"\"\n",
    "        pre-process a document and return a list of tokens (words)\n",
    "        list->list\"\"\"\n",
    "        wordsList=[]\n",
    "        #keep alphanumeric characters\n",
    "        tokenizer = nltk.RegexpTokenizer(r'[a-zA-Z]{2,}')\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        all_stopwords = stopwords.words('english')\n",
    "        for w in document:\n",
    "            # checks if a word is a stop words\n",
    "            filtired = tokenizer.tokenize(w.lower())\n",
    "            try:\n",
    "                if not filtired[0] in all_stopwords:\n",
    "                    # added the word to the list after lemmatization \n",
    "                    wordsList.append(lemmatizer.lemmatize(filtired[0]))\n",
    "            except:\n",
    "                ...\n",
    "        return wordsList\n",
    "# returns a list of tokenized sentences\n",
    "def tokenize_sentence(document: list, min_length = 1) -> list:\n",
    "        sents = []\n",
    "        for sent in document:\n",
    "            words=[]\n",
    "            #keep alphanumeric characters\n",
    "            tokenizer = nltk.RegexpTokenizer(r'[a-zA-Z]{2,}')\n",
    "            lemmatizer = WordNetLemmatizer()                         \n",
    "            all_stopwords = stopwords.words('english')\n",
    "            for w in sent:\n",
    "                filtired = tokenizer.tokenize(w.lower())\n",
    "                try:\n",
    "                    if not filtired[0] in all_stopwords:\n",
    "                        # added the word to the list after lemmatization \n",
    "                        words.append(lemmatizer.lemmatize(filtired[0]))\n",
    "                except:\n",
    "                    ...\n",
    "            if len(words) >= min_length:\n",
    "                sents.append(np.array(words))\n",
    "            \n",
    "        return sents\n",
    "    \n",
    "print(wordlists.sents(\"norton.txt\")[0],\"\\n\")\n",
    "print(len(tokenize_sentence(wordlists.sents(\"norton.txt\"))))\n",
    "print(tokenize_sentence(wordlists.sents(\"norton.txt\"))[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4a50cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38334\n"
     ]
    }
   ],
   "source": [
    "# this list is only used to callculate most_frequent words\n",
    "tokenized_words = []\n",
    "\n",
    "# this list is used to store the tokenized sents from all docs\n",
    "# tokenized_sents = []\n",
    "\n",
    "for file in wordlists.fileids():\n",
    "    tokenized_words += tokenize_words(wordlists.words(file))\n",
    "    \n",
    "#     tokenized_sents += tokenize_sentence(wordlists.sents(file), 1)\n",
    "\n",
    "print(len(tokenized_words))\n",
    "# print(len(tokenized_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85653f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('use', 353), ('phone', 351), ('one', 337), ('router', 337), ('ipod', 329), ('camera', 322), ('player', 313), ('get', 274), ('battery', 264), ('diaper', 231), ('product', 214), ('work', 211), ('like', 196), ('great', 192), ('time', 186), ('feature', 183), ('problem', 179), ('quality', 176), ('good', 176), ('zen', 175), ('would', 158), ('also', 156), ('sound', 153), ('computer', 150), ('software', 148), ('picture', 141), ('well', 138), ('really', 136), ('micro', 136), ('take', 128), ('easy', 125), ('even', 123), ('thing', 123), ('first', 121), ('used', 120), ('need', 119), ('creative', 118), ('bag', 116), ('want', 115), ('much', 115), ('better', 114), ('mp', 113), ('champ', 113), ('look', 110), ('go', 106), ('size', 106), ('music', 105), ('norton', 104), ('little', 101), ('price', 99)]\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "# callculate most frequent words\n",
    "most_frequent = FreqDist(tokenized_words).most_common(50)\n",
    "print(most_frequent)\n",
    "\n",
    "# create pseudowords\n",
    "pseudowords = []\n",
    "for word in most_frequent:\n",
    "    pseudowords.append(word[0][::-1])\n",
    "\n",
    "#target_words are the 100 words in the matrix \n",
    "target_words = []\n",
    "for word in most_frequent:\n",
    "    target_words.append(word[0])\n",
    "    \n",
    "target_words = target_words + pseudowords\n",
    "print(len(target_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "109c9edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace most frequent words\n",
    "def replace_words(indxes, target_words):\n",
    "    new_sents = deepcopy(tokenized_sents)\n",
    "    \n",
    "    #loop over all most frequent words indexes\n",
    "    for i in range(len(indxes)):\n",
    "        \n",
    "        # shuffle and split the list by half\n",
    "        middel = len(indxes[i])//2\n",
    "        temp_indxes = indxes[i].copy()\n",
    "        shuffle(temp_indxes)\n",
    "    #     print(len(temp_indxes[:middel]), len(indxes[i]))\n",
    "    #     print(temp_indxes[:10])\n",
    "    #     print(indxes[i][:10])\n",
    "        for index in temp_indxes[:middel]:\n",
    "            x , y = index\n",
    "    #         print(i,x, y)\n",
    "            new_sents[x][y] = target_words[i + 50]\n",
    "    \n",
    "    return new_sents\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44bedf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a matrix from word2vec weights\n",
    "def create_matrixX(target_words, model):\n",
    "    matrixX = []\n",
    "    for word in target_words:\n",
    "        matrixX.append(model.wv[word])\n",
    "    \n",
    "    return matrixX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b28f6964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4244\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "# this list is used to store the tokenized sents from all docs\n",
    "tokenized_sents = []\n",
    "\n",
    "for file in wordlists.fileids():    \n",
    "    tokenized_sents += tokenize_sentence(wordlists.sents(file), 1)\n",
    "\n",
    "print(len(tokenized_sents))\n",
    "\n",
    "# \n",
    "# gets most frequent words indexes\n",
    "# \n",
    "indxes = []\n",
    "for word in target_words[:50]:\n",
    "    temp = []\n",
    "    for i in range(len(tokenized_sents)):\n",
    "        for j in np.where(tokenized_sents[i] == word)[0]:\n",
    "#             print(i,j)\n",
    "            temp.append([i,j])\n",
    "            \n",
    "    indxes.append(temp)\n",
    "\n",
    "# print(tokenized_sents[1])\n",
    "print(len(indxes))\n",
    "\n",
    "# checks if indxes is correct\n",
    "# for i in range(50):\n",
    "#     if len(indxes[i]) != most_frequent[i][1]:\n",
    "#         print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9bd601f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['sd', 'yllaer', 'enjoyed', 'shooting', 'canon', 'powershot', 'sd'], ['design', 'exterior', 'design', 'combine', 'form', 'function', 'elegantly', 'point', 'shoot', 'ever', 'tested'], ['image', 'processing', 'system', 'digic', 'ii', 'powered', 'image', 'processing', 'system', 'enables', 'sd', 'snap', 'limitless', 'stream', 'megapixel', 'photo', 'respectable', 'clip', 'start', 'emit', 'top', 'class', 'delivers', 'decent', 'photo', 'compared', 'competition']]\n"
     ]
    }
   ],
   "source": [
    "# replace target_words\n",
    "new_sents = replace_words(indxes, target_words)\n",
    "\n",
    "# convert numpy array to normal python list\n",
    "for i in range(len(new_sents)):\n",
    "    new_sents[i] = new_sents[i].tolist()\n",
    "\n",
    "print(new_sents[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90513450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "empty clusters:  0\n",
      "correct pairs:  28\n",
      "p:  56.00000000000001 %\n",
      "\n",
      "empty clusters:  0\n",
      "correct pairs:  23\n",
      "p:  46.0 %\n",
      "\n",
      "empty clusters:  0\n",
      "correct pairs:  26\n",
      "p:  52.0 %\n",
      "\n",
      "empty clusters:  0\n",
      "correct pairs:  29\n",
      "p:  57.99999999999999 %\n",
      "\n",
      "empty clusters:  0\n",
      "correct pairs:  27\n",
      "p:  54.0 %\n",
      "\n",
      "empty clusters:  0\n",
      "correct pairs:  28\n",
      "p:  56.00000000000001 %\n",
      "\n",
      "empty clusters:  0\n",
      "correct pairs:  30\n",
      "p:  60.0 %\n",
      "\n",
      "empty clusters:  0\n",
      "correct pairs:  25\n",
      "p:  50.0 %\n",
      "\n",
      "empty clusters:  0\n",
      "correct pairs:  22\n",
      "p:  44.0 %\n",
      "\n",
      "empty clusters:  0\n",
      "correct pairs:  29\n",
      "p:  57.99999999999999 %\n",
      "\n",
      "mean:  53.4\n",
      "standard deviation:  5.0635955604688645\n"
     ]
    }
   ],
   "source": [
    "# repeat the experiment  10 time\n",
    "N = 10\n",
    "pList = []\n",
    "for i in range(N):\n",
    "    # replace target_words\n",
    "    new_sents = replace_words(indxes, target_words)\n",
    "    for i in range(len(new_sents)):\n",
    "        new_sents[i] = new_sents[i].tolist()\n",
    "\n",
    "    # creates word2vec model\n",
    "    model = Word2Vec(new_sents, min_count = 11, vector_size = 500, window = 7, sg=1)\n",
    "    model.wv.most_similar('use', topn=10)\n",
    "\n",
    "    # creates a matrix from word2vec weights\n",
    "    matrixX = create_matrixX(target_words, model)\n",
    "    \n",
    "    # clester the matrix\n",
    "    kmeans = KMeans(n_clusters=50, random_state=0).fit(matrixX)\n",
    "    pred = kmeans.labels_\n",
    "#     print(pred[:50])\n",
    "#     print(pred[50:])\n",
    "\n",
    "    # checks for empty clusters\n",
    "    empty_clusters = []\n",
    "    for i in range(50):\n",
    "        if i not in pred:\n",
    "            empty_clusters.append(i)\n",
    "\n",
    "    print(\"\\nempty clusters: \", len(empty_clusters))\n",
    "#     print(empty_clusters)\n",
    "\n",
    "\n",
    "    # count correct clustring \n",
    "    count = 0\n",
    "    for i in range(50):\n",
    "        if(pred[i]==pred[i+50]):\n",
    "    #         print(i, pred[i],pred[i+50])\n",
    "            count+=1\n",
    "    \n",
    "    print(\"correct pairs: \",count)\n",
    "    p = (count/50)*100\n",
    "    print(\"p: \", p, \"%\")\n",
    "    \n",
    "    pList.append(p)\n",
    "\n",
    "    \n",
    "print()\n",
    "mean = np.mean(pList)\n",
    "std = np.std(pList)\n",
    "print(\"mean: \", mean)\n",
    "print(\"standard deviation: \", std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18f76d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "998250af",
   "metadata": {},
   "source": [
    "# Testing window size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0f65530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "size:  1\n",
      "mean:  21.4\n",
      "standard deviation:  4.8207883172775805\n",
      "\n",
      "size:  2\n",
      "mean:  42.8\n",
      "standard deviation:  3.9191835884530843\n",
      "\n",
      "size:  3\n",
      "mean:  52.2\n",
      "standard deviation:  3.1559467676119017\n",
      "\n",
      "size:  4\n",
      "mean:  56.0\n",
      "standard deviation:  4.289522117905443\n",
      "\n",
      "size:  5\n",
      "mean:  56.0\n",
      "standard deviation:  4.6475800154488995\n",
      "\n",
      "size:  6\n",
      "mean:  59.8\n",
      "standard deviation:  6.029925372672534\n",
      "\n",
      "size:  7\n",
      "mean:  57.8\n",
      "standard deviation:  6.660330322138685\n",
      "\n",
      "size:  8\n",
      "mean:  52.2\n",
      "standard deviation:  3.3999999999999995\n",
      "\n",
      "size:  9\n",
      "mean:  50.6\n",
      "standard deviation:  4.004996878900159\n",
      "\n",
      "size:  10\n",
      "mean:  50.6\n",
      "standard deviation:  4.651881339845203\n",
      "\n",
      "size:  11\n",
      "mean:  49.6\n",
      "standard deviation:  4.363484845854286\n",
      "\n",
      "size:  12\n",
      "mean:  46.6\n",
      "standard deviation:  6.069596362197408\n"
     ]
    }
   ],
   "source": [
    "# testing best windo size from 1 to 12\n",
    "for w in range(1,13):\n",
    "    # repeat the experiment  10 time\n",
    "    N = 10\n",
    "    pList = []\n",
    "    for i in range(N):\n",
    "        # replace target_words\n",
    "        new_sents = replace_words(indxes, target_words)\n",
    "        for i in range(len(new_sents)):\n",
    "            new_sents[i] = new_sents[i].tolist()\n",
    "\n",
    "        model = Word2Vec(new_sents, vector_size = 500, window = w, sg=1)\n",
    "        model.wv.most_similar('use', topn=10)\n",
    "\n",
    "        matrixX = create_matrixX(target_words, model)\n",
    "        kmeans = KMeans(n_clusters=50, random_state=0).fit(matrixX)\n",
    "        pred = kmeans.labels_\n",
    "\n",
    "        count = 0\n",
    "        for i in range(50):\n",
    "            if(pred[i]==pred[i+50]):\n",
    "                count+=1\n",
    "\n",
    "        p = (count/50)*100    \n",
    "        pList.append(p)\n",
    "\n",
    "\n",
    "    print()\n",
    "    print(\"size: \",w)\n",
    "    mean = np.mean(pList)\n",
    "    std = np.std(pList)\n",
    "    print(\"mean: \", mean)\n",
    "    print(\"standard deviation: \", std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccffe761",
   "metadata": {},
   "source": [
    "# Testing vector size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "019c37f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "size:  10\n",
      "mean:  36.8\n",
      "standard deviation:  6.939740629158989\n",
      "\n",
      "size:  20\n",
      "mean:  47.0\n",
      "standard deviation:  5.0\n",
      "\n",
      "size:  30\n",
      "mean:  50.4\n",
      "standard deviation:  5.043808085167396\n",
      "\n",
      "size:  40\n",
      "mean:  57.0\n",
      "standard deviation:  7.113367697511496\n",
      "\n",
      "size:  50\n",
      "mean:  51.8\n",
      "standard deviation:  4.237924020083421\n",
      "\n",
      "size:  60\n",
      "mean:  54.6\n",
      "standard deviation:  4.737087712930805\n",
      "\n",
      "size:  70\n",
      "mean:  57.4\n",
      "standard deviation:  4.386342439892262\n",
      "\n",
      "size:  80\n",
      "mean:  52.8\n",
      "standard deviation:  4.4\n",
      "\n",
      "size:  90\n",
      "mean:  55.2\n",
      "standard deviation:  5.306599664568639\n",
      "\n",
      "size:  100\n",
      "mean:  57.0\n",
      "standard deviation:  5.0\n",
      "\n",
      "size:  110\n",
      "mean:  57.2\n",
      "standard deviation:  4.1182520563948\n",
      "\n",
      "size:  120\n",
      "mean:  57.4\n",
      "standard deviation:  4.386342439892262\n"
     ]
    }
   ],
   "source": [
    "# testing best vector size from 10 to 120\n",
    "for w in range(1,13):\n",
    "    # repeat the experiment  10 time\n",
    "    N = 10\n",
    "    pList = []\n",
    "    for i in range(N):\n",
    "        # replace target_words\n",
    "        new_sents = replace_words(indxes, target_words)\n",
    "        for i in range(len(new_sents)):\n",
    "            new_sents[i] = new_sents[i].tolist()\n",
    "\n",
    "        model = Word2Vec(new_sents, vector_size = (w)*10, window = 7, sg=1)\n",
    "        model.wv.most_similar('use', topn=10)\n",
    "\n",
    "        matrixX = create_matrixX(target_words, model)\n",
    "        kmeans = KMeans(n_clusters=50, random_state=0).fit(matrixX)\n",
    "        pred = kmeans.labels_\n",
    "\n",
    "        count = 0\n",
    "        for i in range(50):\n",
    "            if(pred[i]==pred[i+50]):\n",
    "                count+=1\n",
    "\n",
    "        p = (count/50)*100    \n",
    "        pList.append(p)\n",
    "\n",
    "\n",
    "    print()\n",
    "    print(\"size: \",w*10)\n",
    "    mean = np.mean(pList)\n",
    "    std = np.std(pList)\n",
    "    print(\"mean: \", mean)\n",
    "    print(\"standard deviation: \", std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c0265f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "size:  100\n",
      "mean:  56.2\n",
      "standard deviation:  4.044749683231337\n",
      "\n",
      "size:  200\n",
      "mean:  59.0\n",
      "standard deviation:  4.02492235949962\n",
      "\n",
      "size:  300\n",
      "mean:  56.6\n",
      "standard deviation:  8.296987405076615\n",
      "\n",
      "size:  400\n",
      "mean:  54.6\n",
      "standard deviation:  5.8\n",
      "\n",
      "size:  500\n",
      "mean:  54.6\n",
      "standard deviation:  5.219195340279955\n",
      "\n",
      "size:  600\n",
      "mean:  55.2\n",
      "standard deviation:  2.9933259094191507\n",
      "\n",
      "size:  700\n",
      "mean:  53.4\n",
      "standard deviation:  4.983974317750845\n",
      "\n",
      "size:  800\n",
      "mean:  54.6\n",
      "standard deviation:  5.370288632839021\n",
      "\n",
      "size:  900\n",
      "mean:  53.6\n",
      "standard deviation:  4.630334761116088\n",
      "\n",
      "size:  1000\n",
      "mean:  55.4\n",
      "standard deviation:  7.432361670424818\n",
      "\n",
      "size:  1100\n",
      "mean:  50.8\n",
      "standard deviation:  3.2496153618543855\n",
      "\n",
      "size:  1200\n",
      "mean:  54.0\n",
      "standard deviation:  6.752777206453653\n"
     ]
    }
   ],
   "source": [
    "# testing best vector size from 100 to 1200\n",
    "for w in range(1,13):\n",
    "    # repeat the experiment  10 time\n",
    "    N = 10\n",
    "    pList = []\n",
    "    for i in range(N):\n",
    "        # replace target_words\n",
    "        new_sents = replace_words(indxes, target_words)\n",
    "        for i in range(len(new_sents)):\n",
    "            new_sents[i] = new_sents[i].tolist()\n",
    "\n",
    "        model = Word2Vec(new_sents, vector_size = (w)*100, window = 7, sg=1)\n",
    "        model.wv.most_similar('use', topn=10)\n",
    "\n",
    "        matrixX = create_matrixX(target_words, model)\n",
    "        kmeans = KMeans(n_clusters=50, random_state=0).fit(matrixX)\n",
    "        pred = kmeans.labels_\n",
    "\n",
    "        count = 0\n",
    "        for i in range(50):\n",
    "            if(pred[i]==pred[i+50]):\n",
    "                count+=1\n",
    "\n",
    "        p = (count/50)*100    \n",
    "        pList.append(p)\n",
    "\n",
    "\n",
    "    print()\n",
    "    print(\"size: \",w*100)\n",
    "    mean = np.mean(pList)\n",
    "    std = np.std(pList)\n",
    "    print(\"mean: \", mean)\n",
    "    print(\"standard deviation: \", std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f4c199",
   "metadata": {},
   "source": [
    "## Testing min word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ae8be8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "size:  1\n",
      "mean:  53.2\n",
      "standard deviation:  2.7129319932501086\n",
      "\n",
      "size:  2\n",
      "mean:  55.6\n",
      "standard deviation:  5.571355310873647\n",
      "\n",
      "size:  3\n",
      "mean:  54.0\n",
      "standard deviation:  5.215361924162118\n",
      "\n",
      "size:  4\n",
      "mean:  55.2\n",
      "standard deviation:  4.664761515876241\n",
      "\n",
      "size:  5\n",
      "mean:  53.6\n",
      "standard deviation:  3.4409301068170497\n",
      "\n",
      "size:  6\n",
      "mean:  53.6\n",
      "standard deviation:  1.9595917942265442\n",
      "\n",
      "size:  7\n",
      "mean:  53.2\n",
      "standard deviation:  4.118252056394799\n",
      "\n",
      "size:  8\n",
      "mean:  55.2\n",
      "standard deviation:  2.9933259094191538\n",
      "\n",
      "size:  9\n",
      "mean:  56.0\n",
      "standard deviation:  4.381780460041329\n",
      "\n",
      "size:  10\n",
      "mean:  57.2\n",
      "standard deviation:  2.712931993250107\n",
      "\n",
      "size:  11\n",
      "mean:  54.8\n",
      "standard deviation:  4.118252056394796\n",
      "\n",
      "size:  12\n",
      "mean:  52.4\n",
      "standard deviation:  5.276362383309165\n",
      "\n",
      "size:  13\n",
      "mean:  53.2\n",
      "standard deviation:  4.833218389437829\n",
      "\n",
      "size:  14\n",
      "mean:  54.4\n",
      "standard deviation:  1.4966629547095796\n",
      "\n",
      "size:  15\n",
      "mean:  55.2\n",
      "standard deviation:  6.764613810115105\n",
      "\n",
      "size:  16\n",
      "mean:  54.4\n",
      "standard deviation:  2.653299832284319\n",
      "\n",
      "size:  17\n",
      "mean:  53.2\n",
      "standard deviation:  4.833218389437828\n",
      "\n",
      "size:  18\n",
      "mean:  55.6\n",
      "standard deviation:  4.454211490264018\n",
      "\n",
      "size:  19\n",
      "mean:  58.4\n",
      "standard deviation:  4.63033476111609\n",
      "\n",
      "size:  20\n",
      "mean:  53.2\n",
      "standard deviation:  5.15363949069005\n",
      "\n",
      "size:  21\n",
      "mean:  55.2\n",
      "standard deviation:  7.2221880341071145\n",
      "\n",
      "size:  22\n",
      "mean:  58.0\n",
      "standard deviation:  5.215361924162119\n",
      "\n",
      "size:  23\n",
      "mean:  56.0\n",
      "standard deviation:  2.828427124746189\n",
      "\n",
      "size:  24\n",
      "mean:  60.4\n",
      "standard deviation:  2.332380757938123\n"
     ]
    }
   ],
   "source": [
    "# testing min word count from 1 to 25\n",
    "for w in range(1,25):\n",
    "    # repeat the experiment  10 time\n",
    "    N = 5\n",
    "    pList = []\n",
    "    for i in range(N):\n",
    "        # replace target_words\n",
    "        new_sents = replace_words(indxes, target_words)\n",
    "        for i in range(len(new_sents)):\n",
    "            new_sents[i] = new_sents[i].tolist()\n",
    "\n",
    "        model = Word2Vec(new_sents, min_count = w, vector_size = 600, window = 7, sg=1)\n",
    "        model.wv.most_similar('use', topn=10)\n",
    "\n",
    "        matrixX = create_matrixX(target_words, model)\n",
    "        kmeans = KMeans(n_clusters=50, random_state=0).fit(matrixX)\n",
    "        pred = kmeans.labels_\n",
    "\n",
    "        count = 0\n",
    "        for i in range(50):\n",
    "            if(pred[i]==pred[i+50]):\n",
    "                count+=1\n",
    "\n",
    "        p = (count/50)*100    \n",
    "        pList.append(p)\n",
    "\n",
    "\n",
    "    print()\n",
    "    print(\"size: \",w)\n",
    "    mean = np.mean(pList)\n",
    "    std = np.std(pList)\n",
    "    print(\"mean: \", mean)\n",
    "    print(\"standard deviation: \", std)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
